{
 "cells": [
  {
   "cell_type": "code",
   "id": "4e192442-f420-4acf-a81e-7542b4e1cbd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:15.338720Z",
     "start_time": "2025-10-19T04:13:15.335575Z"
    }
   },
   "source": [
    "from unittest import case\n",
    "\n",
    "import torch\n",
    "from joblib.externals.loky import cpu_count\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics.functional import accuracy\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "cell_type": "code",
   "id": "1922201b-47f1-4b29-91e4-62f66577b1ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:15.882661Z",
     "start_time": "2025-10-19T04:13:15.881010Z"
    }
   },
   "source": [
    "import numpy\n"
   ],
   "outputs": [],
   "execution_count": 158
  },
  {
   "cell_type": "code",
   "id": "5c11136b-9775-4699-b956-311a17723bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:16.421125Z",
     "start_time": "2025-10-19T04:13:16.418263Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import pickle\n"
   ],
   "outputs": [],
   "execution_count": 159
  },
  {
   "cell_type": "code",
   "id": "0a1e4ce1-6e20-4d34-bc24-a10223db522a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:16.990915Z",
     "start_time": "2025-10-19T04:13:16.985689Z"
    }
   },
   "source": [
    "from decimal import Decimal\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreProcessed:\n",
    "    train_index_start:int\n",
    "    val_index_start:int\n",
    "    test_index_start:int\n",
    "    train_index_end:int\n",
    "    val_index_end:int\n",
    "    test_index_end:int\n",
    "    train_set_length:int\n",
    "    val_set_length:int\n",
    "    test_set_length:int\n",
    "    file:str\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'train_index_start':train_index_start,\n",
    "            'val_index_start':val_index_start,\n",
    "            'test_index_start':test_index_start,\n",
    "            'train_index_end':train_index_end,\n",
    "            'val_index_end':val_index_end,\n",
    "            'test_index_end':test_index_end,\n",
    "            'train_set_length':train_set_length,\n",
    "            'val_set_length':val_set_length,\n",
    "            'test_set_length':test_set_length,\n",
    "            'file':file\n",
    "        }\n",
    "    def __getstate__(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "\n",
    "class PreProcessedEncoder(json.JSONEncoder):\n",
    "        def default(self, o):\n",
    "            if dataclasses.is_dataclass(o):\n",
    "                return dataclasses.asdict(o)\n",
    "            if isinstance(o, Decimal):\n",
    "                return str(o)\n",
    "            return super().default(o)\n"
   ],
   "outputs": [],
   "execution_count": 160
  },
  {
   "cell_type": "code",
   "id": "037c38e1-7dab-4819-be7e-34483be68410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:18.283591Z",
     "start_time": "2025-10-19T04:13:18.280707Z"
    }
   },
   "source": [
    "a = [1,2,3,4]\n",
    "max(a)\n",
    "json.dumps([PreProcessed(1,2,3,4,5,6,7,8,9,\"aaaa\")], cls=PreProcessedEncoder)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"train_index_start\": 1, \"val_index_start\": 2, \"test_index_start\": 3, \"train_index_end\": 4, \"val_index_end\": 5, \"test_index_end\": 6, \"train_set_length\": 7, \"val_set_length\": 8, \"test_set_length\": 9, \"file\": \"aaaa\"}]'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 161
  },
  {
   "cell_type": "code",
   "id": "a752e83f-b07a-40a1-b3e3-b0b249070509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:18.886135Z",
     "start_time": "2025-10-19T04:13:18.879175Z"
    }
   },
   "source": [
    "def convert_to_sequences(memory, days_prediction, data_sequence):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data_sequence) - memory - max(days_prediction)):\n",
    "        intermediate_y = []\n",
    "        window = data_sequence[i:i+memory]\n",
    "        for k in range(memory):\n",
    "            prediction = []\n",
    "            for j in range(len(days_prediction)) :\n",
    "                after_days = days_prediction[j]\n",
    "                prediction.append(data_sequence[i + k  + after_days - 1,[8,9]])\n",
    "            after_window = np.hstack(prediction)\n",
    "            intermediate_y.append(after_window)\n",
    "        x.append(window)\n",
    "        y.append(intermediate_y)\n",
    "    return np.asarray(x), np.asarray(y)\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 162
  },
  {
   "cell_type": "code",
   "id": "999a5b49-2d40-4bc6-914d-e094122353ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:19.761911Z",
     "start_time": "2025-10-19T04:13:19.757442Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3df7330f-a842-41f2-92a8-80fa51b4c6fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:20.444206Z",
     "start_time": "2025-10-19T04:13:20.441749Z"
    }
   },
   "source": [
    "def populate_scaler(csv_directory, cache_directory):\n",
    "\n",
    "    jumbo_df = pd.DataFrame()\n",
    "    for f in tqdm(os.listdir(csv_directory)):\n",
    "        file = os.path.join(csv_directory, f)\n",
    "        df = pd.read_csv(file, sep=',', index_col=False)\n",
    "        jumbo_df = pd.concat([jumbo_df, df])\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(jumbo_df)\n",
    "    joblib.dump(scaler, os.path.join(cache_directory, \"scaler.gz\"))"
   ],
   "outputs": [],
   "execution_count": 163
  },
  {
   "cell_type": "code",
   "id": "686fe7d1-fa48-43af-9e66-04e8591615e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:22.401581Z",
     "start_time": "2025-10-19T04:13:21.106863Z"
    }
   },
   "source": [
    "populate_scaler(\"/data/datasets/stockPredictor/outputs\", \"/data/datasets/stockPredictor/cache\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/75 [00:00<?, ?it/s]\u001B[A\n",
      " 15%|█▍        | 11/75 [00:00<00:00, 107.47it/s]\u001B[A\n",
      " 29%|██▉       | 22/75 [00:00<00:00, 85.96it/s] \u001B[A\n",
      " 41%|████▏     | 31/75 [00:00<00:00, 76.87it/s]\u001B[A\n",
      " 52%|█████▏    | 39/75 [00:00<00:00, 68.15it/s]\u001B[A\n",
      " 61%|██████▏   | 46/75 [00:00<00:00, 64.99it/s]\u001B[A\n",
      " 71%|███████   | 53/75 [00:00<00:00, 60.65it/s]\u001B[A\n",
      " 80%|████████  | 60/75 [00:00<00:00, 55.03it/s]\u001B[A\n",
      " 88%|████████▊ | 66/75 [00:01<00:00, 52.89it/s]\u001B[A\n",
      "100%|██████████| 75/75 [00:01<00:00, 59.64it/s]\u001B[A\n"
     ]
    }
   ],
   "execution_count": 164
  },
  {
   "cell_type": "code",
   "id": "6073fa34-d665-44c2-be06-48d7544f37da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:23.350341Z",
     "start_time": "2025-10-19T04:13:23.343506Z"
    }
   },
   "source": [
    "def load_data(directory, cache_directory, memory, train_perc, val_perc, device, forecast_days = [1, 7, 15]):\n",
    "    rv = []\n",
    "    cache_json_file = os.path.join(cache_directory, \"preprocessed.json\")\n",
    "    if os.path.exists(cache_json_file):\n",
    "        with open(cache_json_file, \"r\") as f :\n",
    "            for o in json.load(f):\n",
    "                v = PreProcessed(**o)\n",
    "                rv.append(PreProcessed(**o))\n",
    "        return rv;\n",
    "    train_input_dataset = []\n",
    "    train_output_dataset = []\n",
    "    val_input_dataset = []\n",
    "    val_output_dataset = []\n",
    "    test_input_dataset = []\n",
    "    test_output_dataset = []\n",
    "    count = 1\n",
    "    train_data_size = 0\n",
    "    val_data_size = 0\n",
    "    test_data_size = 0\n",
    "    train_index_start = 0\n",
    "    val_index_start = 0\n",
    "    test_index_start = 0\n",
    "    for f in tqdm(os.listdir(directory)):\n",
    "        file = os.path.join(directory, f)\n",
    "        df = pd.read_csv(file, sep=',', index_col=False)\n",
    "        input,output = convert_to_sequences(memory, forecast_days, df.to_numpy())\n",
    "        train_size = int(len(input) * train_perc);\n",
    "        val_size = int(len(input) * val_perc);\n",
    "        test_size = int(len(input) - train_size - val_size)\n",
    "        train_input_dataset_single, val_input_dataset_single, test_input_dataset_single = random_split(input, [train_size, val_size, test_size])\n",
    "        train_output_dataset_single, val_output_dataset_single, test_output_dataset_single = random_split(output, [train_size, val_size, test_size])\n",
    "        train_input_dataset = np.vstack([train_input_dataset, train_input_dataset_single]) if (len(train_input_dataset) != 0) else train_input_dataset_single \n",
    "        train_output_dataset = np.vstack([train_output_dataset, train_output_dataset_single]) if (len(train_output_dataset) != 0) else train_output_dataset_single \n",
    "        val_input_dataset = np.vstack([val_input_dataset, val_input_dataset_single]) if (len(val_input_dataset) != 0) else val_input_dataset_single \n",
    "        val_output_dataset = np.vstack([val_output_dataset, val_output_dataset_single]) if (len(val_output_dataset) != 0) else val_output_dataset_single \n",
    "        test_input_dataset = np.vstack([test_input_dataset, test_input_dataset_single]) if (len(test_input_dataset) != 0) else test_input_dataset_single \n",
    "        test_output_dataset = np.vstack([test_output_dataset, test_output_dataset_single]) if (len(test_output_dataset) != 0) else test_output_dataset_single\n",
    "        train_data_size = len(train_input_dataset)\n",
    "        val_data_size = len(val_input_dataset)\n",
    "        test_data_size = len(test_input_dataset)\n",
    "        if (train_data_size > 20000):\n",
    "            print(\"Writing file of size \", train_data_size)\n",
    "            outfile = os.path.join(cache_directory, 'preprocessed-{}.npz'.format(count))\n",
    "            np.savez_compressed(outfile, train_input=np.asarray(train_input_dataset), train_output=np.asarray(train_output_dataset), val_input=np.asarray(val_input_dataset), val_output=np.asarray(val_output_dataset), test_input=np.asarray(test_input_dataset), test_output=np.asarray(test_output_dataset)) \n",
    "            rv.append(PreProcessed(train_index_start, val_index_start, test_index_start, train_index_start + train_data_size - 1, val_index_start + val_data_size - 1, test_index_start + test_data_size - 1, train_data_size, val_data_size, test_data_size, outfile))\n",
    "            train_index_start = train_index_start + train_data_size\n",
    "            val_index_start = val_index_start + val_data_size\n",
    "            test_index_start = test_index_start + test_data_size\n",
    "            train_data_size = 0\n",
    "            val_data_size = 0\n",
    "            test_data_size = 0\n",
    "            count = count + 1\n",
    "            train_input_dataset = []\n",
    "            train_output_dataset = []\n",
    "            val_input_dataset = []\n",
    "            val_output_dataset = []\n",
    "            test_input_dataset = []\n",
    "            test_output_dataset = []\n",
    "    with open(cache_json_file, \"w\") as f :\n",
    "        json.dump(rv, f,cls=PreProcessedEncoder)\n",
    "    return rv"
   ],
   "outputs": [],
   "execution_count": 165
  },
  {
   "cell_type": "code",
   "id": "8927fa26-e8ce-45e3-8d4e-c9567620c235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:24.502789Z",
     "start_time": "2025-10-19T04:13:24.499157Z"
    }
   },
   "source": [
    "a = 4\n",
    "i = 0 if a else 45\n",
    "print(i)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "cell_type": "code",
   "id": "c9e08180-ae96-4d3d-93a6-adc112d9fc73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:25.176548Z",
     "start_time": "2025-10-19T04:13:25.174413Z"
    }
   },
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    TRAIN = 1,\n",
    "    VALIDATE = 2,\n",
    "    TEST = 3"
   ],
   "outputs": [],
   "execution_count": 167
  },
  {
   "cell_type": "code",
   "id": "2110ba6b-7c12-4364-abcc-4d7a6ffb6fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:25.987601Z",
     "start_time": "2025-10-19T04:13:25.984498Z"
    }
   },
   "source": [
    "def findBucketIndex(idx, dataset_type:DatasetType, sequence):\n",
    "    match dataset_type:\n",
    "        case DatasetType.TRAIN:\n",
    "            for i in range(0, len(sequence)):\n",
    "                if idx >= sequence[i].train_index_start and idx <= sequence[i].train_index_end:\n",
    "                    return i\n",
    "            return len(sequence) - 1\n",
    "        case DatasetType.VALIDATE: \n",
    "            for i in range(0, len(sequence)):\n",
    "                if idx >= sequence[i].val_index_start and idx <= sequence[i].val_index_end:\n",
    "                    return i\n",
    "            return len(sequence) - 1\n",
    "        case DatasetType.TEST: \n",
    "            for i in range(0, len(sequence)):\n",
    "                if idx >= sequence[i].test_index_start and idx <= sequence[i].test_index_end:\n",
    "                    return i\n",
    "            return len(sequence) - 1"
   ],
   "outputs": [],
   "execution_count": 168
  },
  {
   "cell_type": "code",
   "id": "6d064aa35c6e5a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:26.646225Z",
     "start_time": "2025-10-19T04:13:26.639866Z"
    }
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class StockPredictorDataset(Dataset) :\n",
    "    def __init__(self, base_directory, dataset_type:DatasetType, sequences: list[PreProcessed],device) :\n",
    "        self.base_directory = base_directory\n",
    "        self.sequences = sequences\n",
    "        self.dataset_type = dataset_type\n",
    "        self.length = 0\n",
    "        self.current_bucket_index = 0\n",
    "        self.device = device\n",
    "        for p in sequences:\n",
    "            match self.dataset_type:\n",
    "                case DatasetType.TRAIN:\n",
    "                    self.length = max(p.train_index_end, self.length)\n",
    "                case DatasetType.VALIDATE:\n",
    "                    self.length = max(p.val_index_end, self.length)\n",
    "                case DatasetType.TEST:\n",
    "                    self.length = max(p.test_index_end, self.length)\n",
    "        self.length += 1\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_dataset_bucket = self.sequences[self.current_bucket_index]\n",
    "        current_bucket_data = np.load(self.current_dataset_bucket.file)\n",
    "        match self.dataset_type:\n",
    "            case DatasetType.TRAIN:\n",
    "                if (idx < current_dataset_bucket.train_index_start or idx > current_dataset_bucket.train_index_end):\n",
    "                    self.current_bucket_index = findBucketIndex(idx, DatasetType.TRAIN, self.sequences)\n",
    "                    current_dataset_bucket = self.sequences[self.current_bucket_index]\n",
    "                    current_bucket_data = np.load(current_dataset_bucket.file)\n",
    "                index_in_bucket = idx - current_dataset_bucket.train_index_start;\n",
    "                train_ip = torch.from_numpy(current_bucket_data['train_input'][index_in_bucket]).float().to(self.device)\n",
    "                train_op = torch.from_numpy(current_bucket_data['train_output'][index_in_bucket]).float().to(self.device)\n",
    "                return train_ip,train_op, \n",
    "            case DatasetType.VALIDATE:\n",
    "                if (idx < current_dataset_bucket.val_index_start or idx > current_dataset_bucket.val_index_end):\n",
    "                    self.current_bucket_index = findBucketIndex(idx, DatasetType.VALIDATE, self.sequences)\n",
    "                    current_dataset_bucket = self.sequences[self.current_bucket_index]\n",
    "                    current_bucket_data = np.load(current_dataset_bucket.file)\n",
    "                index_in_bucket = idx - current_dataset_bucket.val_index_start;\n",
    "                val_ip = torch.from_numpy(current_bucket_data['val_input'][index_in_bucket]).float().to(self.device)\n",
    "                val_op = torch.from_numpy(current_bucket_data['val_output'][index_in_bucket]).float().to(self.device)\n",
    "                return val_ip,val_op, \n",
    "            case DatasetType.TEST:\n",
    "                if (idx < current_dataset_bucket.test_index_start or idx > current_dataset_bucket.test_index_end):\n",
    "                    self.current_bucket_index = findBucketIndex(idx, DatasetType.TEST, self.sequences)\n",
    "                    current_dataset_bucket = self.sequences[self.current_bucket_index]\n",
    "                    current_bucket_data = np.load(current_dataset_bucket.file)\n",
    "                index_in_bucket = idx - current_dataset_bucket.test_index_start;\n",
    "                test_ip = torch.from_numpy(current_bucket_data['test_input'][index_in_bucket]).float().to(self.device)\n",
    "                test_op =torch.from_numpy(current_bucket_data['test_output'][index_in_bucket]).float().to(self.device)\n",
    "                return test_ip,test_op,"
   ],
   "outputs": [],
   "execution_count": 169
  },
  {
   "cell_type": "code",
   "id": "ed500fff91c5e304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:27.339235Z",
     "start_time": "2025-10-19T04:13:27.330677Z"
    }
   },
   "source": [
    "class StockPreductorDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, base_directory, device, train_batch_size: int = 512, val_batch_size:int = 64, test_batch_size:int = 32):\n",
    "        super().__init__()\n",
    "        self.base_directory = base_directory\n",
    "        self.sequences = load_data(os.path.join(base_directory, 'outputs'), os.path.join(base_directory, 'cache'), 300, 0.70, 0.15, device)\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.device = device;\n",
    "\n",
    "    def __getstate__(self):\n",
    "        self.sequences = None\n",
    "        return self.__dict__\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__ = d\n",
    "        self.sequences = load_data(os.path.join(self.base_directory, 'outputs'), os.path.join(self.base_directory, 'cache'), 300, 0.70, 0.15, device)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = StockPredictorDataset(self.base_directory, DatasetType.TRAIN, self.sequences, self.device)\n",
    "        self.val_dataset = StockPredictorDataset(self.base_directory, DatasetType.VALIDATE, self.sequences, self.device)\n",
    "        self.test_dataset = StockPredictorDataset(self.base_directory, DatasetType.TEST, self.sequences, self.device)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.train_batch_size, shuffle=True, num_workers=19, persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.val_batch_size, shuffle=False, num_workers=19, persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.test_batch_size, shuffle=False, num_workers=1, persistent_workers=True)"
   ],
   "outputs": [],
   "execution_count": 170
  },
  {
   "cell_type": "code",
   "id": "cbc759ccfbcba19a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:28.052075Z",
     "start_time": "2025-10-19T04:13:28.044701Z"
    }
   },
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for positional encoding, suitable for transformer models.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int, dropout=0.1, max_len: int = 5000):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor with positional encoding added.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ],
   "outputs": [],
   "execution_count": 171
  },
  {
   "cell_type": "code",
   "id": "f04b17bc503e9acd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:28.654320Z",
     "start_time": "2025-10-19T04:13:28.649763Z"
    }
   },
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for a generic Transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=31, model_dim=512, num_heads=32, num_layers=3, output_dim=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoder(model_dim, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.decoder = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 172
  },
  {
   "cell_type": "code",
   "id": "ea23e8305cdd4e37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:29.349050Z",
     "start_time": "2025-10-19T04:13:29.344760Z"
    }
   },
   "source": [
    "class StockPredictor(pl.LightningModule):\n",
    "    def __init__(self, device, learning_rate: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.model = TransformerModel().to(device)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, outputs=None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if outputs is not None:\n",
    "            loss = self.criterion(output, outputs)\n",
    "        return output, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, loss = self.common_step(batch, batch_idx)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs, loss = self.common_step(batch, batch_idx)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs, loss = self.common_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        print('Input Shape = ', inputs.shape, ' target shape = ' , targets.shape)\n",
    "        outputs, loss = self(inputs, targets)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        return outputs, loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}"
   ],
   "outputs": [],
   "execution_count": 173
  },
  {
   "cell_type": "code",
   "id": "263ada809925376e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:31.420719Z",
     "start_time": "2025-10-19T04:13:31.416433Z"
    }
   },
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 304033), started 0:18:52 ago. (Use '!kill 304033' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "5cc07040e81c1a779f760fc2224f0025"
     }
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5fd7c1aaecdd9816\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5fd7c1aaecdd9816\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 174
  },
  {
   "cell_type": "code",
   "id": "4f61530879c75fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:33.353753Z",
     "start_time": "2025-10-19T04:13:33.344813Z"
    }
   },
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath = \"checkpoints\", filename=\"best-checkpoint\", save_top_k=1, verbose=True, monitor=\"val_loss\", mode=\"min\")\n",
    "logger = TensorBoardLogger('lightning_logs', name=\"stock_predictor\")\n",
    "\n",
    "trainer = pl.Trainer(logger=logger, callbacks=[checkpoint_callback], max_epochs=500)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 175
  },
  {
   "cell_type": "code",
   "id": "1e67bbc2da06166c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-19T04:13:39.992588Z",
     "start_time": "2025-10-19T04:13:34.700286Z"
    }
   },
   "source": [
    "mp.set_start_method('spawn', force=True)\n",
    "model = StockPredictor(device).to(device)\n",
    "data_module = StockPreductorDataModule(base_directory=\"/data/datasets/stockPredictor\", device=device, train_batch_size=1024, val_batch_size=64, test_batch_size=32)\n",
    "data_module.setup()\n",
    "model.train()\n",
    "trainer.fit(model, data_module)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | TransformerModel | 9.5 M  | train\n",
      "1 | criterion | MSELoss          | 0      | train\n",
      "-------------------------------------------------------\n",
      "9.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.5 M     Total params\n",
      "37.906    Total estimated model params size (MB)\n",
      "39        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^    ^self = reduction.pickle.load(from_parent)^\n",
      "^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "Traceback (most recent call last):\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "  File \"<string>\", line 1, in <module>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "   File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "       exitcode = _main(fd, parent_sentinel)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "      exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "     self = reduction.pickle.load(from_parent)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/vavasthi/anaconda3/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'StockPredictorDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 309257, 309258, 309259, 309260, 309261, 309262, 309263, 309264, 309265, 309266, 309267, 309268, 309269, 309270, 309271, 309272, 309273, 309274, 309275) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mEmpty\u001B[39m                                     Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1275\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._try_get_data\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1274\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1275\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data_queue\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1276\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/lib/python3.11/multiprocessing/queues.py:114\u001B[39m, in \u001B[36mQueue.get\u001B[39m\u001B[34m(self, block, timeout)\u001B[39m\n\u001B[32m    113\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._poll(timeout):\n\u001B[32m--> \u001B[39m\u001B[32m114\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m Empty\n\u001B[32m    115\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._poll():\n",
      "\u001B[31mEmpty\u001B[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[176]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m data_module.setup()\n\u001B[32m      5\u001B[39m model.train()\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:560\u001B[39m, in \u001B[36mTrainer.fit\u001B[39m\u001B[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[39m\n\u001B[32m    558\u001B[39m \u001B[38;5;28mself\u001B[39m.training = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    559\u001B[39m \u001B[38;5;28mself\u001B[39m.should_stop = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m560\u001B[39m \u001B[43mcall\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    561\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[32m    562\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:49\u001B[39m, in \u001B[36m_call_and_handle_interrupt\u001B[39m\u001B[34m(trainer, trainer_fn, *args, **kwargs)\u001B[39m\n\u001B[32m     47\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m trainer.strategy.launcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m     48\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001B[32m---> \u001B[39m\u001B[32m49\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     51\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[32m     52\u001B[39m     _call_teardown_hook(trainer)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:598\u001B[39m, in \u001B[36mTrainer._fit_impl\u001B[39m\u001B[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[39m\n\u001B[32m    591\u001B[39m     download_model_from_registry(ckpt_path, \u001B[38;5;28mself\u001B[39m)\n\u001B[32m    592\u001B[39m ckpt_path = \u001B[38;5;28mself\u001B[39m._checkpoint_connector._select_ckpt_path(\n\u001B[32m    593\u001B[39m     \u001B[38;5;28mself\u001B[39m.state.fn,\n\u001B[32m    594\u001B[39m     ckpt_path,\n\u001B[32m    595\u001B[39m     model_provided=\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[32m    596\u001B[39m     model_connected=\u001B[38;5;28mself\u001B[39m.lightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    597\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m598\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m=\u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    600\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.stopped\n\u001B[32m    601\u001B[39m \u001B[38;5;28mself\u001B[39m.training = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1011\u001B[39m, in \u001B[36mTrainer._run\u001B[39m\u001B[34m(self, model, ckpt_path)\u001B[39m\n\u001B[32m   1006\u001B[39m \u001B[38;5;28mself\u001B[39m._signal_connector.register_signal_handlers()\n\u001B[32m   1008\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m   1009\u001B[39m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[32m   1010\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1011\u001B[39m results = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1013\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m   1014\u001B[39m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[32m   1015\u001B[39m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[32m   1016\u001B[39m log.debug(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m: trainer tearing down\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001B[39m, in \u001B[36mTrainer._run_stage\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.training:\n\u001B[32m   1052\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[32m-> \u001B[39m\u001B[32m1053\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1054\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m torch.autograd.set_detect_anomaly(\u001B[38;5;28mself\u001B[39m._detect_anomaly):\n\u001B[32m   1055\u001B[39m         \u001B[38;5;28mself\u001B[39m.fit_loop.run()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1082\u001B[39m, in \u001B[36mTrainer._run_sanity_check\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1079\u001B[39m call._call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mon_sanity_check_start\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1081\u001B[39m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1082\u001B[39m \u001B[43mval_loop\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1084\u001B[39m call._call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mon_sanity_check_end\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m   1086\u001B[39m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001B[39m, in \u001B[36m_no_grad_context.<locals>._decorator\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    177\u001B[39m     context_manager = torch.no_grad\n\u001B[32m    178\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:138\u001B[39m, in \u001B[36m_EvaluationLoop.run\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    137\u001B[39m     dataloader_iter = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m138\u001B[39m     batch, batch_idx, dataloader_idx = \u001B[38;5;28mnext\u001B[39m(data_fetcher)\n\u001B[32m    139\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m previous_dataloader_idx != dataloader_idx:\n\u001B[32m    140\u001B[39m     \u001B[38;5;66;03m# the dataloader has changed, notify the logger connector\u001B[39;00m\n\u001B[32m    141\u001B[39m     \u001B[38;5;28mself\u001B[39m._store_dataloader_outputs()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:134\u001B[39m, in \u001B[36m_PrefetchDataFetcher.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    131\u001B[39m         \u001B[38;5;28mself\u001B[39m.done = \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.batches\n\u001B[32m    132\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.done:\n\u001B[32m    133\u001B[39m     \u001B[38;5;66;03m# this will run only when no pre-fetching was done.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     batch = \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__next__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    136\u001B[39m     \u001B[38;5;66;03m# the iterator is empty\u001B[39;00m\n\u001B[32m    137\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:61\u001B[39m, in \u001B[36m_DataFetcher.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m     59\u001B[39m \u001B[38;5;28mself\u001B[39m._start_profiler()\n\u001B[32m     60\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m61\u001B[39m     batch = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterator)\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m     63\u001B[39m     \u001B[38;5;28mself\u001B[39m.done = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001B[39m, in \u001B[36mCombinedLoader.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    339\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__next__\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> _ITERATOR_RETURN:\n\u001B[32m    340\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m._iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m341\u001B[39m     out = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m._iterator)\n\u001B[32m    342\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m._iterator, _Sequential):\n\u001B[32m    343\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:142\u001B[39m, in \u001B[36m_Sequential.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    139\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m142\u001B[39m     out = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterators[\u001B[32m0\u001B[39m])\n\u001B[32m    143\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[32m    144\u001B[39m     \u001B[38;5;66;03m# try the next iterator\u001B[39;00m\n\u001B[32m    145\u001B[39m     \u001B[38;5;28mself\u001B[39m._use_next_iterator()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/torch/utils/data/dataloader.py:732\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    729\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    730\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    731\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    734\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    735\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    736\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    738\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1482\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1479\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._process_data(data, worker_id)\n\u001B[32m   1481\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._tasks_outstanding > \u001B[32m0\u001B[39m\n\u001B[32m-> \u001B[39m\u001B[32m1482\u001B[39m idx, data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1483\u001B[39m \u001B[38;5;28mself\u001B[39m._tasks_outstanding -= \u001B[32m1\u001B[39m\n\u001B[32m   1484\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable:\n\u001B[32m   1485\u001B[39m     \u001B[38;5;66;03m# Check for _IterableDatasetStopIteration\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1444\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._get_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1440\u001B[39m     \u001B[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001B[39;00m\n\u001B[32m   1441\u001B[39m     \u001B[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001B[39;00m\n\u001B[32m   1442\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1443\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1444\u001B[39m         success, data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1445\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m success:\n\u001B[32m   1446\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/venv/stockPredictor/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1288\u001B[39m, in \u001B[36m_MultiProcessingDataLoaderIter._try_get_data\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m   1286\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(failed_workers) > \u001B[32m0\u001B[39m:\n\u001B[32m   1287\u001B[39m     pids_str = \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m.join(\u001B[38;5;28mstr\u001B[39m(w.pid) \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m failed_workers)\n\u001B[32m-> \u001B[39m\u001B[32m1288\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1289\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDataLoader worker (pid(s) \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpids_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) exited unexpectedly\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1290\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m   1291\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, queue.Empty):\n\u001B[32m   1292\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[31mRuntimeError\u001B[39m: DataLoader worker (pid(s) 309257, 309258, 309259, 309260, 309261, 309262, 309263, 309264, 309265, 309266, 309267, 309268, 309269, 309270, 309271, 309272, 309273, 309274, 309275) exited unexpectedly"
     ]
    }
   ],
   "execution_count": 176
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "34f657ce9f92dd0e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stockPredictor)",
   "language": "python",
   "name": "stockpredictor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
