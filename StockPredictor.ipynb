{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "4e192442-f420-4acf-a81e-7542b4e1cbd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:18.012957Z",
     "start_time": "2025-10-18T09:49:15.726323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from unittest import case\n",
    "\n",
    "import torch\n",
    "from joblib.externals.loky import cpu_count\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torchmetrics.functional import accuracy\n",
    "\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "1922201b-47f1-4b29-91e4-62f66577b1ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:18.019469Z",
     "start_time": "2025-10-18T09:49:18.017600Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "5c11136b-9775-4699-b956-311a17723bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:21.127657Z",
     "start_time": "2025-10-18T09:49:21.125135Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "0a1e4ce1-6e20-4d34-bc24-a10223db522a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:23.749802Z",
     "start_time": "2025-10-18T09:49:23.747012Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreProcessed:\n",
    "    train_index_start:int\n",
    "    val_index_start:int\n",
    "    test_index_start:int\n",
    "    train_index_end:int\n",
    "    val_index_end:int\n",
    "    test_index_end:int\n",
    "    train_set_length:int\n",
    "    val_set_length:int\n",
    "    test_set_length:int\n",
    "    file:str\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'train_index_start':train_index_start,\n",
    "            'val_index_start':val_index_start,\n",
    "            'test_index_start':test_index_start,\n",
    "            'train_index_end':train_index_end,\n",
    "            'val_index_end':val_index_end,\n",
    "            'test_index_end':test_index_end,\n",
    "            'train_set_length':train_set_length,\n",
    "            'val_set_length':val_set_length,\n",
    "            'test_set_length':test_set_length,\n",
    "            'file':file\n",
    "        }\n",
    "        \n",
    "class PreProcessedEncoder(json.JSONEncoder):\n",
    "        def default(self, o):\n",
    "            if dataclasses.is_dataclass(o):\n",
    "                return dataclasses.asdict(o)\n",
    "            if isinstance(obj, Decimal):\n",
    "                return str(obj)\n",
    "            return super().default(o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "037c38e1-7dab-4819-be7e-34483be68410",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:25.613661Z",
     "start_time": "2025-10-18T09:49:25.607737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"train_index_start\": 1, \"val_index_start\": 2, \"test_index_start\": 3, \"train_index_end\": 4, \"val_index_end\": 5, \"test_index_end\": 6, \"train_set_length\": 7, \"val_set_length\": 8, \"test_set_length\": 9, \"file\": \"aaaa\"}]'"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4]\n",
    "max(a)\n",
    "json.dumps([PreProcessed(1,2,3,4,5,6,7,8,9,\"aaaa\")], cls=PreProcessedEncoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "a752e83f-b07a-40a1-b3e3-b0b249070509",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:35.570025Z",
     "start_time": "2025-10-18T09:49:35.563664Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_sequences(memory, days_prediction, data_sequence):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data_sequence) - memory - max(days_prediction)):\n",
    "        intermediate_y = []\n",
    "        window = data_sequence[i:i+memory]\n",
    "        for k in range(memory):\n",
    "            prediction = []\n",
    "            for j in range(len(days_prediction)) :\n",
    "                after_days = days_prediction[j]\n",
    "                prediction.append(data_sequence[i + k  + after_days - 1,[8,9]])\n",
    "            after_window = np.hstack(prediction)\n",
    "            intermediate_y.append(after_window)\n",
    "        x.append(window)\n",
    "        y.append(intermediate_y)\n",
    "    return np.asarray(x), np.asarray(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a5b49-2d40-4bc6-914d-e094122353ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T05:25:59.316775Z",
     "start_time": "2025-10-18T05:25:59.315399Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "id": "3df7330f-a842-41f2-92a8-80fa51b4c6fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:43.514102Z",
     "start_time": "2025-10-18T09:49:43.500418Z"
    }
   },
   "outputs": [],
   "source": [
    "def populate_scaler(csv_directory, cache_directory):\n",
    "\n",
    "    jumbo_df = pd.DataFrame()\n",
    "    for f in tqdm(os.listdir(csv_directory)):\n",
    "        file = os.path.join(csv_directory, f)\n",
    "        df = pd.read_csv(file, sep=',', index_col=False)\n",
    "        jumbo_df = pd.concat([jumbo_df, df])\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(jumbo_df)\n",
    "    joblib.dump(scaler, os.path.join(cache_directory, \"scaler.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "id": "686fe7d1-fa48-43af-9e66-04e8591615e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:43.514102Z",
     "start_time": "2025-10-18T09:49:43.500418Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                         | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|████████████████▎                                                                                              | 11/75 [00:00<00:00, 105.25it/s]\u001b[A\n",
      " 29%|████████████████████████████████▊                                                                               | 22/75 [00:00<00:00, 82.28it/s]\u001b[A\n",
      " 41%|██████████████████████████████████████████████▎                                                                 | 31/75 [00:00<00:00, 72.98it/s]\u001b[A\n",
      " 52%|██████████████████████████████████████████████████████████▏                                                     | 39/75 [00:00<00:00, 64.90it/s]\u001b[A\n",
      " 61%|████████████████████████████████████████████████████████████████████▋                                           | 46/75 [00:00<00:00, 58.77it/s]\u001b[A\n",
      " 69%|█████████████████████████████████████████████████████████████████████████████▋                                  | 52/75 [00:00<00:00, 57.51it/s]\u001b[A\n",
      " 77%|██████████████████████████████████████████████████████████████████████████████████████▌                         | 58/75 [00:00<00:00, 51.52it/s]\u001b[A\n",
      " 85%|███████████████████████████████████████████████████████████████████████████████████████████████▌                | 64/75 [00:01<00:00, 46.56it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 70/75 [00:01<00:00, 47.61it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 75/75 [00:01<00:00, 54.82it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "populate_scaler(\"/data/datasets/stockPredictor/outputs\", \"/data/datasets/stockPredictor/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "6073fa34-d665-44c2-be06-48d7544f37da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:43.514102Z",
     "start_time": "2025-10-18T09:49:43.500418Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(directory, cache_directory, memory, train_perc, val_perc, device, forecast_days = [1, 7, 15]):\n",
    "    rv = []\n",
    "    cache_json_file = os.path.join(cache_directory, \"preprocessed.json\")\n",
    "    if os.path.exists(cache_json_file):\n",
    "        with open(cache_json_file, \"r\") as f :\n",
    "            for o in json.load(f):\n",
    "                v = PreProcessed(**o)\n",
    "                rv.append(PreProcessed(**o))\n",
    "        return rv;\n",
    "    train_input_dataset = []\n",
    "    train_output_dataset = []\n",
    "    val_input_dataset = []\n",
    "    val_output_dataset = []\n",
    "    test_input_dataset = []\n",
    "    test_output_dataset = []\n",
    "    count = 1\n",
    "    train_data_size = 0\n",
    "    val_data_size = 0\n",
    "    test_data_size = 0\n",
    "    train_index_start = 0\n",
    "    val_index_start = 0\n",
    "    test_index_start = 0\n",
    "    for f in tqdm(os.listdir(directory)):\n",
    "        file = os.path.join(directory, f)\n",
    "        df = pd.read_csv(file, sep=',', index_col=False)\n",
    "        input,output = convert_to_sequences(memory, forecast_days, df.to_numpy())\n",
    "        train_size = int(len(input) * train_perc);\n",
    "        val_size = int(len(input) * val_perc);\n",
    "        test_size = int(len(input) - train_size - val_size)\n",
    "        train_input_dataset_single, val_input_dataset_single, test_input_dataset_single = random_split(input, [train_size, val_size, test_size])\n",
    "        train_output_dataset_single, val_output_dataset_single, test_output_dataset_single = random_split(output, [train_size, val_size, test_size])\n",
    "        train_input_dataset = np.vstack([train_input_dataset, train_input_dataset_single]) if (len(train_input_dataset) != 0) else train_input_dataset_single \n",
    "        train_output_dataset = np.vstack([train_output_dataset, train_output_dataset_single]) if (len(train_output_dataset) != 0) else train_output_dataset_single \n",
    "        val_input_dataset = np.vstack([val_input_dataset, val_input_dataset_single]) if (len(val_input_dataset) != 0) else val_input_dataset_single \n",
    "        val_output_dataset = np.vstack([val_output_dataset, val_output_dataset_single]) if (len(val_output_dataset) != 0) else val_output_dataset_single \n",
    "        test_input_dataset = np.vstack([test_input_dataset, test_input_dataset_single]) if (len(test_input_dataset) != 0) else test_input_dataset_single \n",
    "        test_output_dataset = np.vstack([test_output_dataset, test_output_dataset_single]) if (len(test_output_dataset) != 0) else test_output_dataset_single\n",
    "        train_data_size = len(train_input_dataset)\n",
    "        val_data_size = len(val_input_dataset)\n",
    "        test_data_size = len(test_input_dataset)\n",
    "        if (train_data_size > 20000):\n",
    "            print(\"Writing file of size \", train_data_size)\n",
    "            outfile = os.path.join(cache_directory, 'preprocessed-{}.npz'.format(count))\n",
    "            np.savez_compressed(outfile, train_input=np.asarray(train_input_dataset), train_output=np.asarray(train_output_dataset), val_input=np.asarray(val_input_dataset), val_output=np.asarray(val_output_dataset), test_input=np.asarray(test_input_dataset), test_output=np.asarray(test_output_dataset)) \n",
    "            rv.append(PreProcessed(train_index_start, val_index_start, test_index_start, train_index_start + train_data_size - 1, val_index_start + val_data_size - 1, test_index_start + test_data_size - 1, train_data_size, val_data_size, test_data_size, outfile))\n",
    "            train_index_start = train_index_start + train_data_size\n",
    "            val_index_start = val_index_start + val_data_size\n",
    "            test_index_start = test_index_start + test_data_size\n",
    "            train_data_size = 0\n",
    "            val_data_size = 0\n",
    "            test_data_size = 0\n",
    "            count = count + 1\n",
    "            train_input_dataset = []\n",
    "            train_output_dataset = []\n",
    "            val_input_dataset = []\n",
    "            val_output_dataset = []\n",
    "            test_input_dataset = []\n",
    "            test_output_dataset = []\n",
    "    with open(cache_json_file, \"w\") as f :\n",
    "        json.dump(rv, f,cls=PreProcessedEncoder)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "3bb35d0b-a55e-457e-94aa-ffdc8bed5fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:49.549659Z",
     "start_time": "2025-10-18T09:49:48.584966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22301, 300, 31) (22301, 300, 6)\n"
     ]
    }
   ],
   "source": [
    "arr = np.load(os.path.join('/data/datasets/stockPredictor/cache', 'preprocessed-1.npz'))\n",
    "print(arr['train_input'].shape, arr['train_output'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "8927fa26-e8ce-45e3-8d4e-c9567620c235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:49:58.772416Z",
     "start_time": "2025-10-18T09:49:58.770296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = 4\n",
    "i = 0 if a else 45\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "c9e08180-ae96-4d3d-93a6-adc112d9fc73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:50:05.110658Z",
     "start_time": "2025-10-18T09:50:05.108573Z"
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class DatasetType(Enum):\n",
    "    TRAIN = 1,\n",
    "    VALIDATE = 2,\n",
    "    TEST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "2110ba6b-7c12-4364-abcc-4d7a6ffb6fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:50:06.837133Z",
     "start_time": "2025-10-18T09:50:06.831526Z"
    }
   },
   "outputs": [],
   "source": [
    "def findBucketIndex(idx, dataset_type:DatasetType, sequence):\n",
    "    match dataset_type:\n",
    "        case DatasetType.TRAIN:\n",
    "            for i in range(0, len(sequence)):\n",
    "                if idx >= sequence[i].train_index_start and idx <= sequence[i].train_index_end:\n",
    "                    return i\n",
    "            return len(sequence) - 1\n",
    "        case DatasetType.VALIDATE: \n",
    "            for i in range(0, len(sequence)):\n",
    "                if idx >= sequence[i].val_index_start and idx <= sequence[i].val_index_end:\n",
    "                    return i\n",
    "            return len(sequence) - 1\n",
    "        case DatasetType.TEST: \n",
    "            for i in range(0, len(sequence)):\n",
    "                if idx >= sequence[i].test_index_start and idx <= sequence[i].test_index_end:\n",
    "                    return i\n",
    "            return len(sequence) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "6d064aa35c6e5a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:50:06.837133Z",
     "start_time": "2025-10-18T09:50:06.831526Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class StockPredictorDataset(Dataset) :\n",
    "    def __init__(self, base_directory, dataset_type:DatasetType, sequences: list[PreProcessed],device) :\n",
    "        self.base_directory = base_directory\n",
    "        self.sequences = sequences\n",
    "        self.dataset_type = dataset_type\n",
    "        self.length = 0\n",
    "        self.current_bucket_index = 0\n",
    "        self.current_dataset_bucket = sequences[self.current_bucket_index]\n",
    "        self.current_bucket_data = np.load(self.current_dataset_bucket.file)\n",
    "        self.device = device;\n",
    "        print('Dataset the length of sequences is ', len(sequences))\n",
    "        for p in sequences:\n",
    "            match self.dataset_type:\n",
    "                case DatasetType.TRAIN:\n",
    "                    self.length = max(p.train_index_end, self.length)\n",
    "                case DatasetType.VALIDATE:\n",
    "                    self.length = max(p.val_index_end, self.length)\n",
    "                case DatasetType.TEST:\n",
    "                    self.length = max(p.test_index_end, self.length)\n",
    "        self.length += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        match self.dataset_type:\n",
    "            case DatasetType.TRAIN:\n",
    "                if (idx < self.current_dataset_bucket.train_index_start or idx > self.current_dataset_bucket.train_index_end):\n",
    "                    self.current_bucket_index = findBucketIndex(idx, DatasetType.TRAIN, self.sequences)\n",
    "                    self.current_dataset_bucket = self.sequences[self.current_bucket_index]\n",
    "                    self.current_bucket_data = np.load(self.current_dataset_bucket.file)\n",
    "                index_in_bucket = idx - self.current_dataset_bucket.train_index_start;\n",
    "                train_ip = torch.from_numpy(self.current_bucket_data['train_input'][index_in_bucket]).float().to(self.device)\n",
    "                train_op = torch.from_numpy(self.current_bucket_data['train_output'][index_in_bucket]).float().to(self.device)\n",
    "                return train_ip,train_op, \n",
    "            case DatasetType.VALIDATE:\n",
    "                if (idx < self.current_dataset_bucket.val_index_start or idx > self.current_dataset_bucket.val_index_end):\n",
    "                    self.current_bucket_index = findBucketIndex(idx, DatasetType.VALIDATE, self.sequences)\n",
    "                    self.current_dataset_bucket = self.sequences[self.current_bucket_index]\n",
    "                    self.current_bucket_data = np.load(self.current_dataset_bucket.file)\n",
    "                index_in_bucket = idx - self.current_dataset_bucket.val_index_start;\n",
    "                val_ip = torch.from_numpy(self.current_bucket_data['val_input'][index_in_bucket]).float().to(self.device)\n",
    "                val_op = torch.from_numpy(self.current_bucket_data['val_output'][index_in_bucket]).float().to(self.device)\n",
    "                return val_ip,val_op, \n",
    "            case DatasetType.TEST:\n",
    "                if (idx < self.current_dataset_bucket.test_index_start or idx > self.current_dataset_bucket.test_index_end):\n",
    "                    self.current_bucket_index = findBucketIndex(idx, DatasetType.TEST, self.sequences)\n",
    "                    self.current_dataset_bucket = self.sequences[self.current_bucket_index]\n",
    "                    self.current_bucket_data = np.load(self.current_dataset_bucket.file)\n",
    "                index_in_bucket = idx - self.current_dataset_bucket.test_index_start;\n",
    "                val_ip = torch.from_numpy(self.current_bucket_data['test_input'][index_in_bucket]).float().to(self.device)\n",
    "                val_op =torch.from_numpy(self.current_bucket_data['test_output'][index_in_bucket]).float().to(self.device)\n",
    "                return test_ip,test_op, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "ed500fff91c5e304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T10:16:40.721818Z",
     "start_time": "2025-10-18T10:16:40.715764Z"
    }
   },
   "outputs": [],
   "source": [
    "class StockPreductorDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, base_directory, device, batch_size: int = 512):\n",
    "        super().__init__()\n",
    "        self.base_directory = base_directory\n",
    "        self.sequences = load_data('/data/datasets/stockPredictor/outputs', '/data/datasets/stockPredictor/cache', 300, 0.70, 0.15, device)\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device;\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = StockPredictorDataset(self.base_directory, DatasetType.TRAIN, self.sequences, self.device)\n",
    "        self.val_dataset = StockPredictorDataset(self.base_directory, DatasetType.VALIDATE, self.sequences, self.device)\n",
    "        self.test_dataset = StockPredictorDataset(self.base_directory, DatasetType.TEST, self.sequences, self.device)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "id": "cbc759ccfbcba19a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:50:16.903155Z",
     "start_time": "2025-10-18T09:50:16.899023Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for positional encoding, suitable for transformer models.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model:int, dropout=0.1, max_len: int = 5000):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Tensor with positional encoding added.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "f04b17bc503e9acd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:50:25.037171Z",
     "start_time": "2025-10-18T09:50:25.034219Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module for a generic Transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=31, model_dim=512, num_heads=32, num_layers=3, output_dim=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoder(model_dim, dropout)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.decoder = nn.Linear(model_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "4c6bfd7401e8aa4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:08:20.617472Z",
     "start_time": "2025-10-18T09:08:20.569413Z"
    }
   },
   "outputs": [],
   "source": [
    "model = TransformerModel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "ea23e8305cdd4e37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:50:35.609415Z",
     "start_time": "2025-10-18T09:50:35.600071Z"
    }
   },
   "outputs": [],
   "source": [
    "class StockPredictor(pl.LightningModule):\n",
    "    def __init__(self, device, learning_rate: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.model = TransformerModel().to(device)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x, outputs=None):\n",
    "        output = self.model(x)\n",
    "        loss = 0\n",
    "        if outputs is not None:\n",
    "            loss = self.criterion(output, outputs)\n",
    "        return output, loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, loss = self.common_step(batch, batch_idx)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs, loss = self.common_step(batch, batch_idx)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs, loss = self.common_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        print('Input Shape = ', inputs.shape, ' target shape = ' , targets.shape)\n",
    "        outputs, loss = self(inputs, targets)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        return outputs, loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "cb6dfa1c269650c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:57:49.385183Z",
     "start_time": "2025-10-18T09:57:49.191634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StockPredictor(\n",
       "  (model): TransformerModel(\n",
       "    (embedding): Linear(in_features=31, out_features=512, bias=True)\n",
       "    (pos_encoder): PositionalEncoder(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-2): 3 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (criterion): MSELoss()\n",
       "    (decoder): Linear(in_features=512, out_features=6, bias=True)\n",
       "  )\n",
       "  (criterion): MSELoss()\n",
       ")"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StockPredictor(device).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "263ada809925376e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:46:53.595716Z",
     "start_time": "2025-10-18T09:46:53.591214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bb88e12f36144cad\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bb88e12f36144cad\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "4f61530879c75fee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T09:54:17.719925Z",
     "start_time": "2025-10-18T09:54:17.695434Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath = \"checkpoints\", filename=\"best-checkpoint\", save_top_k=1, verbose=True, monitor=\"val_loss\", mode=\"min\")\n",
    "logger = TensorBoardLogger('lightning_logs', name=\"stock_predictor\")\n",
    "\n",
    "trainer = pl.Trainer(logger=logger, callbacks=[checkpoint_callback], max_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "1e67bbc2da06166c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T10:23:48.255855Z",
     "start_time": "2025-10-18T10:17:30.665579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset the length of sequences is  12\n",
      "Dataset the length of sequences is  12\n",
      "Dataset the length of sequences is  12\n"
     ]
    }
   ],
   "source": [
    "data_module = StockPreductorDataModule(base_directory=\"/data/datasets/stockPredictor/cache\", device=device, batch_size=512)\n",
    "data_module.setup()\n",
    "dl = data_module.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafbb5df3c8738b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T10:14:21.019663Z",
     "start_time": "2025-10-18T10:14:20.832119Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | TransformerModel | 9.5 M  | train\n",
      "1 | criterion | MSELoss          | 0      | train\n",
      "-------------------------------------------------------\n",
      "9.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.5 M     Total params\n",
      "37.906    Total estimated model params size (MB)\n",
      "39        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset the length of sequences is  12\n",
      "Dataset the length of sequences is  12\n",
      "Dataset the length of sequences is  12\n",
      "Sanity Checking: |                                                                                                             | 0/? [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105ba36-46f1-44fc-8b75-d6cdec3ba0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e81cffb-4c73-4408-a143-ef6f2ab8fde9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (stockPredictor)",
   "language": "python",
   "name": "stockpredictor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
